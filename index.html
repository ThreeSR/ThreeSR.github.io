<!-- <!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<title></title>
	</head>
	<body>
	</body>
</html> -->

<!DOCTYPE htm1>
<html>
	<body>
		<h3>Hello World</h3>
		<p>
			My name is Rui Sun. I received my bachelor's degree from <a href="https://en.xidian.edu.cn/">Xidian University</a>. 
			Then, I received my master's degree from <a href="https://www.columbia.edu/">Columbia University</a> (<a href="https://www.ee.columbia.edu/ms-concentrations">Research Specialization</a>, Advisor: <a href="https://scholar.google.com/citations?user=OMVTRscAAAAJ&hl=en">Prof. Shih-Fu Chang</a>). 
			My research interests are Vision-Language Multimodal Learning, Computer Vision, and Natural Language Processing.
		</p>
		<p>
			<a href="https://github.com/ThreeSR">[GitHub]</a> 
			<a href="https://www.linkedin.com/in/rui-sun-999717173">[LinkedIn]</a> 
			<a href="https://www.semanticscholar.org/author/Rui-Sun/2068172926">[Semantic Scholar]</a>
			<a href="rs4110@columbia.edu">[Email]</a>
		</p>
		<h3>Publication</h3>
		<p>
			IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models<br>
			Findings of EMNLP 2023 (long)<br>
			Haoxuan You*, Rui Sun*, Zhecan Wang*, Long Chen, Gengyu Wang, Hammad A. Ayyubi, Kai-Wei Chang, Shih-Fu Chang<br>
			[<a href="https://arxiv.org/pdf/2305.14985.pdf">Paper</a>][<a href="https://github.com/Hxyou/IdealGPT">Code</a>]<br>
			<br>
			UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding<br>
			Findings of ACL 2023 (long)<br>
			Rui Sun*, Zhecan Wang*, Haoxuan You*, Noel Codella, Kai-Wei Chang, Shih-Fu Chang<br>
			[<a href="https://aclanthology.org/2023.findings-acl.49/">Paper</a>][<a href="https://github.com/ThreeSR/UniFine">Code</a>]<br>
			<br>
			Find Someone Who: Visual Commonsense Understanding in Human-Centric Grounding<br>
			Findings of EMNLP 2022 (long)<br>
			Haoxuan You, Rui Sun, Zhecan Wang, Kai-Wei Chang, Shih-Fu Chang<br>
			[<a href="https://aclanthology.org/2022.findings-emnlp.399/">Paper</a>][<a href="https://github.com/Hxyou/HumanCog">Code</a>]<br>
			<br>
			<b>Technical Report</b><br>
			An empirical study of QA-oriented pretraining<br>
			Rui Sun<br>
			[<a href="https://github.com/ThreeSR/QA-Oriented-Pretraining">GitHub Repo</a>]
		</p>
		<h3>Experience</h3>
		<p>
			<a href="http://www1.cs.columbia.edu/nlp/index.cgi">Columbia NLP</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?user=jee2Dy0AAAAJ&hl=en">Prof. Zhou Yu</a>, Sep 2023 -- Present<br>
			Currently working on Multimodal Dialog System<br>
			<br>
			<a href="https://mitibmwatsonailab.mit.edu/">MIT-IBM Watson AI Lab</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?user=QSRdIzAAAAAJ&hl=zh-CN">Dr. Zhenfang Chen</a>, <a href="https://scholar.google.com/citations?user=PTeSCbIAAAAJ">Prof. Chuang Gan</a>, Jun 2023 -- Present<br>
			Neuro-Symbolic Visual Reasoning (One co-first author paper in submission)<br>
			<br>
			<a href="https://rizzolab.org/">Rehab Engineering Alliance & Center Transforming Low Vision, NYU Langone Health</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?user=TmPZ5uEAAAAJ&hl=en">Prof. JohnRoss Rizzo</a>, <a href="https://scholar.google.com/citations?user=jee2Dy0AAAAJ&hl=en">Prof. Zhou Yu</a>, Jun 2023 -- Aug 2023<br>
			Deploy Vision-Language & Large Language Models in wearable devices for Blindness and Low Vision People<br>
			<br>
			<a href="https://www.ee.columbia.edu/ln/dvmm/">Digital Video and Multimedia (DVMM) Lab, Columbia University</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?user=BhysChMAAAAJ&hl=en">Haoxuan You</a>, <a href="https://scholar.google.com/citations?user=uqHPnmgAAAAJ&hl=en">Zhecan Wang</a>, <a href="https://scholar.google.com/citations?user=OMVTRscAAAAJ&hl=en">Prof. Shih-Fu Chang</a>, <a href="https://scholar.google.com/citations?user=fqDBtzYAAAAJ&hl=en">Prof. Kai-Wei Chang</a>, Oct 2021 -- Jun 2023<br>
			Large Language Model Aided Visual Reasoning (Findings of EMNLP 2023)<br>
			Zero-shot Vision-Language Understanding (Findings of ACL 2023)<br>
			Human-centric Visual Commonsense Grounding Dataset (Findings of EMNLP 2022)<br>
			Task-oriented Pretraining (Second-stage Pretraining) of Vision-Language Pretrained Models (Technical Report)<br>
			<br>
			<a href="https://isn.xidian.edu.cn/">State Key Laboratory of Integrated Services Networks, Xidian University</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?user=mcIEFDMAAAAJ&hl=zh-CN">Prof. Jingwei Xin</a>, <a href="https://scholar.google.com/citations?user=SRBn7oUAAAAJ&hl">Prof. Nannan Wang</a>, Sep 2020 -- Jul 2021<br>
			Human Face Frontalization and Hallucination (More details can be found in <a href="https://ieeexplore.ieee.org/document/10115425">this paper</a>)<br>
		</p>
		<h3>Honors & Awards</h3>
		<p>
			Interdisciplinary Contest in Modeling (ICM), Meritorious Winner, 2018
		</p>
		<h3>Service</h3>
		<p>
			<b>Conference Reviewer</b><br>
			EMNLP 2023, AAAI 2024, ACL Rolling Review (ARR)<br>
			<br>
			<b>Teaching Assistant at Columbia</b><br>
			COMS 4995 Deep Learning for Computer Vision, working with Prof. Peter Belhumeur, Fall 2023<br>
			ELEN 4815 Random Signals and Noise, worked with Prof. Irving Kalet, Spring 2023<br>
			COMS 4995 Neural Networks & Deep Learning, worked with Prof. Richard Zemel, Fall 2022<br>
			COMS 4732 Computer Vision II, worked with Prof. Carl Vondrick, Spring 2022<br>
		</p>

<!-- 2023/10/9: 彩蛋：如果你能通过网页源代码看到我的这句话，我希望你在生活和学术上都一切顺利，一帆风顺~ :) -->
<!-- 2023/10/9: Easter egg: If you can see this sentence by viewing the source code of my homepage, I wish you all the best in your life and academia ~ :) -->
<!-- 		<a href='https://clustrmaps.com/site/1bwuf'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=350&t=n&d=CY53cNKmeCLi9H5dBQeh2tIAkA_59a_qE7bYGFcDRrI'/></a> -->
		<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=CY53cNKmeCLi9H5dBQeh2tIAkA_59a_qE7bYGFcDRrI'></script>
	</body>
</html>
