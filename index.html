<!-- <!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<title></title>
	</head>
	<body>
	</body>
</html> -->

<!-- Update Logs -->
<!-- 2023/10/9: 彩蛋：如果你能通过网页源代码看到我的这句话，我希望你在生活和学术上都一切顺利，一帆风顺~ :) -->
<!-- 2023/10/9: Easter egg: If you can see this sentence by viewing the source code of my homepage, I wish you all the best in your life and academia ~ :) -->

<!-- 2023/11/8: Get ready to start PhD application. Good luck to me!) -->

<!-- 2023/11/12: Add GENOME to publication) -->

<!-- 2023/12/14: Update some outdated information) -->

<!-- 2024/01/17: Update some outdated information && ICLR 2024 Accepted) -->

<!-- 2024/05/07: Update some outdated information) -->

<!-- 2024/05/07: Update some outdated information) -->

<!-- 2024/08/23: Update some outdated information and my status; One paper was accepted by ACM MM 2024; No photo update, need to do it next time -->

<!-- I really appreciate the assistance from Haoxuan, Zhecan, and Zhenfang. They are super nice mentors and teach me a lot. -->
<!-- In my free time, I often run (~6-10 km, 4-6.5 miles). I also like Karaoke, Chess, and going to museums. -->

<!--
News Archive

More news is on the way...

2024/07: One paper was accepted by ACM MM 2024; Got US Visa for PhD journey

2024/05: One paper was released on ArXiv

2024/04: Accepted UCLA CS PhD Offer

2024/03: Received CS PhD Offer from UCLA

2024/01: One paper was accepted by ICLR 2024; TA for Math of Deep Learning

2023/12: Begin my PhD application again

2023/11: One paper was released on ArXiv

2023/10: One paper was accepted by Findings of EMNLP 2023

2023/09: TA for Deep Learning for Computer Vision

2023/07: Start a project working with NYU Langone

2023/06: Feel fortunate to work with Zhenfang

2023/06: One paper was released on ArXiv

2023/05: One paper was accepted by Findings of ACL 2023; my previous project was accepted by IEEE TCSVT (congrats to Jingwei and Zikai); Graduate from Columbia and Start my Gap Year

2023/02: Submit my very first first-authored paper to ACL 2023

2023/01: TA for Random Signal and Noise

2022/12: Begin my PhD application

2022/10: One paper was accepted by Findings of EMNLP 2022.

2022/09: TA for Neural Network and Deep Learning

2022/01: Work as Teaching Assistant for Computer Vision II

2021/10: Work as Research Assistant at Columbia DVMM Lab advised by Dr. Haoxuan You, Dr. Zhecan Wang, and Prof. Shih-Fu Chang

2021/09: Start my master's program at Columbia EE
-->

<!DOCTYPE htm1>
<html>
	<head>
		<style>
		.float-left {
			float: left;
			margin-right: 20px; /* 添加一些右边距，以便文本不会紧贴图片 */
		}
		.clearfix::after {
			content: "";
			clear: both;
			display: table;
		}
		</style>
		<title>Rui Sun</title>
		<!-- <link rel="icon" type="image/png" href="favicon.png"> -->
		<link rel="icon" type="image/x-icon" href="fig/Columbia_Engineering.png">
	</head>
	<body>
		<h3>Hello World</h3>
		<p>
			My name is Rui Sun (Rui is pronounced as Ray). I am a CS PhD student at <a href="https://web.cs.ucla.edu/~kwchang/">UCLA NLP</a>. I received my bachelor's degree from <a href="https://en.xidian.edu.cn/">Xidian University</a>. 
			Then, I received my master's degree from <a href="https://www.columbia.edu/">Columbia University</a> (<a href="https://www.ee.columbia.edu/ms-concentrations">Research Specialization</a>, Advisor: <a href="https://scholar.google.com/citations?user=OMVTRscAAAAJ&hl=en">Prof. Shih-Fu Chang</a>). 
			My research interests are Vision-Language Multimodal Learning, Natural Language Processing, and Computer Vision.<br>
			<br>
			I am fortunate enough to work with <b>brilliant</b> mentors and advisors. You can find my work experience and their names in <a href="#exp">Experience</a> section.<br>
			<br>
			Multimodal Learning is a broad topic, if you would like to know more about what I am doing and what I did, please jump to <a href="#pub">Publication</a> or <a href="#exp">Experience</a> section.
		</p>
		<p>
			<a href="https://github.com/ThreeSR">[GitHub]</a> 
			<a href="https://www.linkedin.com/in/rui-sun-999717173">[LinkedIn]</a> 
			<a href="https://www.semanticscholar.org/author/Rui-Sun/2068172926">[Semantic Scholar]</a>
			<a href="https://twitter.com/RuiSun94013021">[Twitter]</a>
			<a href="mailto:ruis@g.ucla.edu">[Email]</a><br>
			<!-- I maintain a list of Multimodal Learning papers <a href="https://github.com/ThreeSR/Paper-Reading">HERE</a>. Let's dive into Multimodal Learning together. -->
		</p>
		<!-- announcement -->
		<p>
			<!-- <strong style="color: red;">I am actively looking for PhD opportunities(24 Fall) and Research Interns(24 Spring and 24 Summer). If you believe I can be a good fit, please <a href="mailto:rs4110@columbia.edu">Email</a> me to let me know. Thanks~</strong> -->
			<!-- <strong style="color: red;">I am actively looking for PhD opportunities(24 Fall) and Research Interns(24 Spring and 24 Summer). If you believe I can be a good fit, please <a href="mailto:rs4110@columbia.edu">Email</a> me to let me know. Thanks~</strong> -->
			<strong style="color: red;">I am actively looking for Research Intern opportunities in 2025. If you believe I can be a good fit, please <a href="mailto:ruis@g.ucla.edu">Email</a> me to let me know. Thanks~</strong>
		</p>
		<p>
			If you would like to know more about me, you could view the source code of this webpage for more information. (Right-click the page and select View Page Source)
		</p>
		<p>
			<!-- Last Update: 12/14/2023 -->
			<!-- Last Update: 5/7/2024<br> -->
			<!-- Last Update: 8/23/2024<br> -->
			<!-- Last Update: 9/28/2024<br> -->
			<!-- Last Update: 10/2/2024<br> -->
			Last Update: 10/24/2024<br>
			<br>
			<!-- <b>News:</b><br>
			<u><a href="https://arxiv.org/abs/2409.12953">One paper</a> on Vision and Language Understanding Evaluation is accepted by NeurIPS 2024 Datasets and Benchmarks Track. Congrats to <a href="https://scholar.google.com/citations?user=uqHPnmgAAAAJ&hl=en">Zhecan Wang!</a></u><br> -->
			<!-- <u><a href="https://arxiv.org/abs/2405.11145">One paper</a> on Vision Language Models Hallucination Mitigation was accepted by ACM MM 2024. Congrats to <a href="https://openreview.net/profile?id=~Junzhang_Liu1">Junzhang Liu!</a></u> -->
			<!-- <b>News:</b> <u>One paper on Neuro-Symbolic Visual Reasoning is accepted by ICLR 2024.</u> -->
		</p>
		<h3 id="pub">Publications</h3>
		* denotes equal contribution
		<p>
			<img src="fig/armap.jpg" alt="JB" style="width: 400px; height: 200px;"><br>
			<b>Autonomous Agents from Automatic Reward Modeling and Planning</b><br>
			Zhenfang Chen*, Delin Chen*, <strong>Rui Sun*</strong>, Wenjun Liu*, Chuang Gan<br>
			To be released<br>
			<!-- [<a href="https://journeybench.github.io/">Project Page</a>][<a href="https://arxiv.org/pdf/2409.12953">Paper</a>][<a href="https://github.com/JourneyBench/JourneyBench">Code</a>]<br> -->
			<br>
			<img src="fig/journey_bench.jpg" alt="JB" style="width: 400px; height: 200px;"><br>
			<b>JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images</b><br>
			NeurIPS 2024 Datasets and Benchmarks Track<br>
			Zhecan Wang*, Junzhang Liu*, Chia-Wei Tang, Hani Alomari, Anushka Sivakumar, <strong>Rui Sun</strong>, Wenhao Li, Md. Atabuzzaman, Hammad Ayyubi, Haoxuan You, Alvi Md Ishmam, Kai-Wei Chang, Shih-Fu Chang, Chris Thomas<br>
			[<a href="https://journeybench.github.io/">Project Page</a>][<a href="https://arxiv.org/pdf/2409.12953">Paper</a>][<a href="https://github.com/JourneyBench/JourneyBench">Code</a>]<br>
			<br>
			<img src="fig/cara.jpg" alt="CARA" style="width: 400px; height: 167px;"><br>
			<b>Detecting Multimodal Situations with Insufficient Context and Abstaining from Baseless Predictions</b><br>
			ACM MM 2024<br>
			Junzhang Liu*, Zhecan Wang*, Hammad Ayyubi*, Haoxuan You, Chris Thomas, <strong>Rui Sun</strong>, Shih-Fu Chang, Kai-Wei Chang<br>
			[<a href="https://drive.google.com/file/d/1zX_IFUT9PtEanUW314ci-aaWlD6NVlmL/view?usp=sharing">Paper</a>][<a href="https://github.com/JunzhangLiu/CARA">Code</a>]<br>
			<br>
			<img src="fig/genome.jpg" alt="GENOME" style="width: 400px; height: 200px;"><br>
			<!-- <video width="320" height="200" controls>
				<source src="https://drive.google.com/file/d/1MbTWM1QypP-aog15BeO6LxDg346r7f1I/view" type="video/mp4">
				Your browser doesn't support video label.
			</video><br> -->
			<b>GENOME: Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules</b><br>
			ICLR 2024<br>
			Zhenfang Chen*, <strong>Rui Sun*</strong>, Wenjun Liu*, Yining Hong, Chuang Gan<br>
			[<a href="https://vis-www.cs.umass.edu/genome/">Project Page</a>][<a href="https://drive.google.com/file/d/1MbTWM1QypP-aog15BeO6LxDg346r7f1I/view?usp=drive_link">Video Demo</a>][<a href="https://openreview.net/forum?id=MNShbDSxKH">Paper</a>][<a href="https://github.com/UMass-Foundation-Model/genome">Code</a>]<br>
			<!-- </div> -->
			<br>
			<!-- <div class="clearfix">
			<img src="fig/idealgpt.jpg" alt="IdealGPT" style="width: 132px; height: 66px;" class="float-left"><br> -->
			<img src="fig/idealgpt.jpg" alt="IdealGPT" style="width: 400px; height: 200px;"><br>
			<b>IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models</b><br>
			Findings of EMNLP 2023 (long)<br>
			Haoxuan You*, <strong>Rui Sun*</strong>, Zhecan Wang*, Long Chen, Gengyu Wang, Hammad A. Ayyubi, Kai-Wei Chang, Shih-Fu Chang<br>
			[<a href="https://aclanthology.org/2023.findings-emnlp.755.pdf">Paper</a>][<a href="https://github.com/Hxyou/IdealGPT">Code</a>]<br>
			<!-- </div> -->
			<br>
			<img src="fig/unifine.jpg" alt="UniFine" style="width: 287px; height: 284px;"><br>
			<b>UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding</b><br>
			Findings of ACL 2023 (long)<br>
			<strong>Rui Sun*</strong>, Zhecan Wang*, Haoxuan You*, Noel Codella, Kai-Wei Chang, Shih-Fu Chang<br>
			[<a href="https://aclanthology.org/2023.findings-acl.49/">Paper</a>][<a href="https://github.com/ThreeSR/UniFine">Code</a>]<br>
			<br>
			<img src="fig/vcg.jpg" alt="VCG" style="width: 400px; height: 167px;"><br>
			<b>Find Someone Who: Visual Commonsense Understanding in Human-Centric Grounding</b><br>
			Findings of EMNLP 2022 (long)<br>
			Haoxuan You, <strong>Rui Sun</strong>, Zhecan Wang, Kai-Wei Chang, Shih-Fu Chang<br>
			[<a href="https://aclanthology.org/2022.findings-emnlp.399/">Paper</a>][<a href="https://github.com/Hxyou/HumanCog">Code</a>]<br>
			<br>
			<b>Technical Report</b><br>
			<img src="fig/second_pretrain.jpg" alt="Second_Pretrain" style="width: 400px; height: 86px;"><br>
			<b>An empirical study of QA-oriented pretraining</b><br>
			<strong>Rui Sun</strong><br>
			[<a href="https://github.com/ThreeSR/QA-Oriented-Pretraining">GitHub Repo</a>]
		</p>
		<h3 id="exp">Experience</h3>
		<p>
			<a href="https://web.cs.ucla.edu/~kwchang/members/">UCLA NLP</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?hl=en&user=fqDBtzYAAAAJ">Prof. Kai-Wei Chang</a>, Sep 2024 -- Present<br>
			Text-to-Video Generation <br>
			Language Agent <br>
			<br>
			<a href="https://mitibmwatsonailab.mit.edu/">MIT-IBM Watson AI Lab</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?user=QSRdIzAAAAAJ&hl=zh-CN">Dr. Zhenfang Chen</a>, <a href="https://scholar.google.com/citations?user=PTeSCbIAAAAJ">Prof. Chuang Gan</a>, Jun 2023 -- Oct 2024<br>
			Monte Carlo Tree Search for Language Agent Planning (Under Review)<br>
			Neuro-Symbolic Visual Reasoning (ICLR 2024)<br>
			<br>
			<a href="https://www.ee.columbia.edu/ln/dvmm/">Digital Video and Multimedia (DVMM) Lab, Columbia University</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?user=BhysChMAAAAJ&hl=en">Dr. Haoxuan You</a>, <a href="https://scholar.google.com/citations?user=uqHPnmgAAAAJ&hl=en">Dr. Zhecan Wang</a>, <a href="https://scholar.google.com/citations?user=OMVTRscAAAAJ&hl=en">Prof. Shih-Fu Chang</a>, <a href="https://scholar.google.com/citations?user=fqDBtzYAAAAJ&hl=en">Prof. Kai-Wei Chang</a>, Oct 2021 -- Jun 2023, Oct 2023 -- Jun 2024<br>
			Vision and Language Understanding Evaluation (NeurIPS 2024 Datasets and Benchmarks Track)<br>
			Vision Language Models Hallucination Mitigation (ACM MM 2024)<br>
			Large Language Model Aided Visual Reasoning (Findings of EMNLP 2023)<br>
			Zero-shot Vision-Language Understanding (Findings of ACL 2023)<br>
			Human-centric Visual Commonsense Grounding Dataset (Findings of EMNLP 2022)<br>
			Task-oriented Pretraining (Second-stage Pretraining) of Vision-Language Pretrained Models (Technical Report)<br>
			<br>
			<a href="http://www1.cs.columbia.edu/nlp/index.cgi">Columbia NLP</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?user=jee2Dy0AAAAJ&hl=en">Prof. Zhou Yu</a>, Sep 2023 -- Mar 2024<br>
			<!-- Currently working on Model Editing and Fine-grained and Explainable Visual Categorization<br> -->
			Fine-grained and Explainable Visual Categorization<br>
			<br>
			<a href="https://rizzolab.org/">Rehab Engineering Alliance & Center Transforming Low Vision, NYU Langone Health</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?user=TmPZ5uEAAAAJ&hl=en">Prof. JohnRoss Rizzo</a>, <a href="https://scholar.google.com/citations?user=jee2Dy0AAAAJ&hl=en">Prof. Zhou Yu</a>, Jun 2023 -- Aug 2023<br>
			AI for Social Good: Deploy Vision-Language & Large Language Models in wearable devices for Blindness and Low Vision People (More details can be found in <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2236097&HistoricalAwards=false">this page</a>)<br>
			<br>
			<a href="https://iip-xdu.github.io/index.html">Intelligent Information Processing (IIP) Lab, Xidian University</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?user=mcIEFDMAAAAJ&hl=zh-CN">Prof. Jingwei Xin</a>, <a href="https://scholar.google.com/citations?user=SRBn7oUAAAAJ&hl">Prof. Nannan Wang</a>, Sep 2020 -- Jul 2021<br>
			Human Face Frontalization and Hallucination (More details can be found in <a href="https://ieeexplore.ieee.org/document/10115425">this paper</a>)<br>
		</p>
		<p>
			<img src="fig/iip_colorful.png" alt="IIP" style="width: 120px; height: 36px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/dvmm.gif" alt="DVMM" style="width: 128px; height: 36px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/nyu_langone.png" alt="NYU_Langone" style="width: 90px; height: 50px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/mit_ibm.jpg" alt="MIT_IBM" style="width: 50px; height: 50px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/columbia_nlp.jpg" alt="CU_NLP" style="width: 50px; height: 50px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/ucla_nlp.png" alt="UCLA_NLP" style="width: 50px; height: 50px;"><br>
		</p>
		<p>
			<img src="fig/xdu.jpg" alt="Xidian" style="width: 50px; height: 50px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/columbia.jpg" alt="Roar Lions!" style="width: 50px; height: 50px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/ucla.png" alt="Go Bruins!" style="width: 50px; height: 50px;">
		</p>
		<h3>Honors & Awards</h3>
		<p>
			Interdisciplinary Contest in Modeling (ICM), Meritorious Winner, 2018
		</p>
		<h3>Service</h3>
		<p>
			<!-- <b>Conference Reviewer</b><br> -->
			<b>Reviewer</b><br>
			NLP: EMNLP, EACL, NAACL, ACL Rolling Review (ARR), ACL<br>
			AI: AAAI<br>
			Multimedia: ACM MM<br>
			ML: ICLR<br>
			<!-- EMNLP 2023, AAAI 2024, ACL Rolling Review (ARR), EACL 2024, NAACL 2024, ACL 2024, ACM MM 2024<br> -->
			<br>
			<b>Teaching Assistant at Columbia</b><br>
			EECS 6699 Mathematics of Deep Learning, worked with Prof. Predrag Jelenkovic, Spring 2024<br>
			COMS 4995 Deep Learning for Computer Vision, worked with Prof. Peter Belhumeur, Fall 2023<br>
			ELEN 4815 Random Signals and Noise, worked with Prof. Irving Kalet, Spring 2023<br>
			COMS 4995 Neural Networks & Deep Learning, worked with Prof. Richard Zemel, Fall 2022<br>
			COMS 4732 Computer Vision II, worked with Prof. Carl Vondrick, Spring 2022<br>
		</p>
		<h3>Misc</h3>
		<p>
			<img src="fig/royce_hall2.jpg" alt="Fall" style="width: px; height: 227px;"> &nbsp;&nbsp;&nbsp;
			<img src="fig/irises.jpg" alt="Fall" style="width: px; height: 227px;"> &nbsp;&nbsp;&nbsp;
			<img src="fig/10km.jpg" alt="Fall" style="width: px; height: 227px;"> <br>
			Los Angeles, CA, 2024 Fall, Royce Hall <br>
			Los Angeles, CA, 2024 Fall, Getty Center <br>
			Drake Stadium, CA, 2024 Fall, My Best 10KM Record <br>
			<br>
			<a href="photos.html">Old Photos</a>
		</p>
<!-- 		<a href='https://clustrmaps.com/site/1bwuf'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=350&t=n&d=CY53cNKmeCLi9H5dBQeh2tIAkA_59a_qE7bYGFcDRrI'/></a> -->
		<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=CY53cNKmeCLi9H5dBQeh2tIAkA_59a_qE7bYGFcDRrI'></script>
	</body>
</html>
