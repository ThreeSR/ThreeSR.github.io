<!-- <!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<title></title>
	</head>
	<body>
	</body>
</html> -->

<!-- Update Logs -->
<!-- 2023/10/9: 彩蛋：如果你能通过网页源代码看到我的这句话，我希望你在生活和学术上都一切顺利，一帆风顺~ :) -->
<!-- 2023/10/9: Easter egg: If you can see this sentence by viewing the source code of my homepage, I wish you all the best in your life and academia ~ :) -->

<!-- 2023/11/8: Get ready to start PhD application. Good luck to me!) -->

<!-- 2023/11/12: Add GENOME to publication) -->

<!-- 2023/12/14: Update some outdated information) -->

<!-- 2024/01/17: Update some outdated information && ICLR 2024 Accepted) -->

<!-- 2024/05/07: Update some outdated information) -->

<!-- 2024/05/07: Update some outdated information) -->

<!-- 2024/08/23: Update some outdated information and my status; One paper was accepted by ACM MM 2024; No photo update, need to do it next time -->

<!-- I really appreciate the assistance from Haoxuan and Zhenfang. They are super nice mentors and teach me a lot. -->
<!-- In my free time, I often run (~6-10 km, 4-6.5 miles). I also like Karaoke, Chess, and going to museums. -->

<!--
News Archive

More news is on the way...

2025/06: Joined Deep Learning Group at Microsoft Research, Redmond as a Research Intern, working on Language Agent

2025/06: One paper on Embodied Web Agent was released on ArXiv

2025/06: One proposal on Language Agent security was funded by Google Research Scholar Program (led by Prof. Chris Thomas)

2025/03: One paper on Inference Time Scaling for Language Agent is accepted by ICLR 2025. Look forward to seeing old and new friends in Singapore!

2025/01: One paper was accepted by ICLR 2025; Plan to start research intern at MSR

2024/12: Attend NeurIPS 2024

2024/11: Attend SoCal NLP at UCSD

2024/09: Start PhD at UCLA NLP

2024/07: One paper was accepted by ACM MM 2024; Got US Visa for PhD journey

2024/05: One paper was released on ArXiv

2024/04: Accepted UCLA CS PhD Offer

2024/03: Received CS PhD Offer from UCLA

2024/01: One paper was accepted by ICLR 2024; TA for Math of Deep Learning

2023/12: Begin my PhD application again

2023/11: One paper was released on ArXiv

2023/10: One paper was accepted by Findings of EMNLP 2023

2023/09: TA for Deep Learning for Computer Vision

2023/07: Start a project working with NYU Langone

2023/06: Feel fortunate to work with Zhenfang

2023/06: One paper was released on ArXiv

2023/05: One paper was accepted by Findings of ACL 2023; my previous project was accepted by IEEE TCSVT (congrats to Jingwei and Zikai); Graduate from Columbia and Start my Gap Year

2023/02: Submit my very first first-authored paper to ACL 2023

2023/01: TA for Random Signal and Noise

2022/12: Begin my PhD application

2022/10: One paper was accepted by Findings of EMNLP 2022.

2022/09: TA for Neural Network and Deep Learning

2022/01: Work as Teaching Assistant for Computer Vision II

2021/10: Work as Research Assistant at Columbia DVMM Lab advised by Dr. Haoxuan You, Dr. Zhecan Wang, and Prof. Shih-Fu Chang

2021/09: Start my master's program at Columbia EE
-->

<!DOCTYPE htm1>
<html>
	<head>
		<style>
		.float-left {
			float: left;
			margin-right: 20px; /* 添加一些右边距，以便文本不会紧贴图片 */
		}
		.clearfix::after {
			content: "";
			clear: both;
			display: table;
		}
		</style>
		<title>Rui Sun</title>
		<!-- <link rel="icon" type="image/png" href="favicon.png"> -->
		<link rel="icon" type="image/x-icon" href="fig/ucla_nlp.png">
	</head>
	<body>
		<h3>Hello World</h3>
		<p>
			My name is Rui Sun (Rui is pronounced as Ray). I am a CS PhD student at <a href="https://web.cs.ucla.edu/~kwchang/">UCLA NLP</a> advised by <a href="https://web.cs.ucla.edu/~kwchang/">Prof. Kai-Wei Chang</a>. I received my master's degree from <a href="https://www.columbia.edu/">Columbia University</a> (<a href="https://www.ee.columbia.edu/ms-concentrations">Research Specialization</a>, Advisor: <a href="https://scholar.google.com/citations?user=OMVTRscAAAAJ&hl=en">Prof. Shih-Fu Chang</a>). 
			Prior to that, I received my bachelor's degree from <a href="https://en.xidian.edu.cn/">Xidian University</a>, where I worked with <a href="https://scholar.google.com/citations?user=SRBn7oUAAAAJ&hl">Prof. Nannan Wang</a>.
			My research interests are Vision-Language Multimodal Learning, Natural Language Processing, and Computer Vision.<br>
			<br>
			I am fortunate enough to work with <b>brilliant</b> mentors and advisors. You can find my work experience and their names in <a href="#exp">Experience</a> section.<br>
			<br>
			Multimodal Learning is a broad topic, if you would like to know more about what I am doing and what I did, please jump to <a href="#pub">Publication</a> or <a href="#exp">Experience</a> section.
		</p>
		<p>
			<a href="https://github.com/ThreeSR">[GitHub]</a> 
			<a href="https://www.linkedin.com/in/rui-sun-999717173">[LinkedIn]</a> 
			<a href="https://www.semanticscholar.org/author/Rui-Sun/2068172926">[Semantic Scholar]</a>
			<a href="https://twitter.com/RuiSun94013021">[Twitter]</a>
			<a href="mailto:ruis@g.ucla.edu">[Email]</a> (Please contact me via UCLA email address; I seldom check previous Columbia email)<br>
			<!-- I maintain a list of Multimodal Learning papers <a href="https://github.com/ThreeSR/Paper-Reading">HERE</a>. Let's dive into Multimodal Learning together. -->
		</p>
		<!-- announcement -->
		<p>
			<!-- <strong style="color: red;">I am actively looking for PhD opportunities(24 Fall) and Research Interns(24 Spring and 24 Summer). If you believe I can be a good fit, please <a href="mailto:rs4110@columbia.edu">Email</a> me to let me know. Thanks~</strong> -->
			<!-- <strong style="color: red;">I am actively looking for PhD opportunities(24 Fall) and Research Interns(24 Spring and 24 Summer). If you believe I can be a good fit, please <a href="mailto:rs4110@columbia.edu">Email</a> me to let me know. Thanks~</strong> -->
			<!-- <strong style="color: red;">I am actively looking for Research Intern opportunities in 2025. If you believe I can be a good fit, please <a href="mailto:ruis@g.ucla.edu">Email</a> me to let me know. Thanks~</strong> -->
		</p>
		<p>
			If you would like to know more about me, you could view the source code of this webpage for more information. (Right-click the page and select View Page Source)
		</p>
		<p>
			<!-- Last Update: 12/14/2023 -->
			<!-- Last Update: 5/7/2024<br> -->
			<!-- Last Update: 8/23/2024<br> -->
			<!-- Last Update: 9/28/2024<br> -->
			<!-- Last Update: 10/2/2024<br> -->
			<!-- Last Update: 10/24/2024<br> -->
			<!-- Last Update: 03/02/2025<br> -->
			Last Update: 07/07/2025<br>
			<br>
			<b>Recent News (06/2025):</b> <br>
			<br>
			<u>One paper on <a href="https://embodied-web-agent.github.io/">Embodied Web Agent</a> was released on ArXiv.</u> <strong style="color: red;"> This paper concretizes my current research roadmap: a Multimodal Language Agent should be able to tackle tasks in both the digital world and the real world, freely moving between these environments to achieve unified automation. Feel free to have a look at our work!</strong><br>
			<br>
			<u>One proposal on Language Agent Security was funded by <a href="https://research.google/programs-and-events/research-scholar-program/recipients/?filtertab=2025">Google Research Scholar Program</a> (led by <a href="https://scholar.google.com/citations?user=16r7ZhsAAAAJ">Prof. Chris Thomas</a>)</u><br>
			<br>
			I joined <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/">Microsoft Research, Redmond</a> as a Research Intern. We can meet if you're also in Building 99, haha!
			<!-- <b>News:</b> <u>One paper on Inference Time Scaling for Language Agent is accepted by ICLR 2025. Look forward to seeing old and new friends in Singapore!</u> -->
			<!-- <b>News:</b><br>
			<u><a href="https://arxiv.org/abs/2409.12953">One paper</a> on Vision and Language Understanding Evaluation is accepted by NeurIPS 2024 Datasets and Benchmarks Track. Congrats to <a href="https://scholar.google.com/citations?user=uqHPnmgAAAAJ&hl=en">Zhecan Wang!</a></u><br> -->
			<!-- <u><a href="https://arxiv.org/abs/2405.11145">One paper</a> on Vision Language Models Hallucination Mitigation was accepted by ACM MM 2024. Congrats to <a href="https://openreview.net/profile?id=~Junzhang_Liu1">Junzhang Liu!</a></u> -->
			<!-- <b>News:</b> <u>One paper on Neuro-Symbolic Visual Reasoning is accepted by ICLR 2024.</u> -->
		</p>
		<h3 id="pub">Publications and Preprints</h3>
		*, ** denotes equal contribution (co-first and co-second authorship)<br>
		<p>
			<img src="fig/ewa2.png" alt="ARMAP" style="width: 800px; height: 600px;"><br>
			<b>EMBODIED WEB AGENTS: Bridging Physical-Digital Realms for Integrated Agent Intelligence</b><br>
			ArXiv, in submission<br>
			Yining Hong*, <strong>Rui Sun*</strong>, Bingxuan Li**, Xingcheng Yao**, Maxine Wu**, Alexander Chien**, Da Yin, Ying Nian Wu, Zhecan James Wang, Kai-Wei Chang<br>
			[<a href="https://arxiv.org/abs/2506.15677">Paper</a>]
			[<a href="https://embodied-web-agent.github.io/">Project Page</a>][<a href="https://github.com/Embodied-Web-Agent/Embodied-Web-Agent">Code</a>] [<a href="https://x.com/yining_hong/status/1935520190994997314">Twitter</a>]<br>
			<br>
			<img src="fig/armap.jpg" alt="ARMAP" style="width: 750px; height: 280px;"><br>
			<b>ARMAP: Scaling Autonomous Agents via Automatic Reward Modeling And Planning</b><br>
			ICLR 2025<br>
			Zhenfang Chen*, Delin Chen*, <strong>Rui Sun*</strong>, Wenjun Liu*, Chuang Gan<br>
			[<a href="https://arxiv.org/abs/2502.12130">Paper</a>] [<a href="https://github.com/ThreeSR/Awesome-Inference-Time-Scaling">Paper List of Inference/Test Time Scaling/Computing</a>]
			[<a href="https://armap-agent.github.io/">Project Page</a>][<a href="https://github.com/heaplax/ARMAP">Code</a>] [<a href="https://x.com/RuiSun94013021/status/1892349398274781263">Twitter</a>]<br>
			<br>
			<img src="fig/journey_bench.jpg" alt="JB" style="width: 650px; height: 300px;"><br>
			<b>JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images</b><br>
			NeurIPS 2024 Datasets and Benchmarks Track<br>
			Zhecan Wang*, Junzhang Liu*, Chia-Wei Tang, Hani Alomari, Anushka Sivakumar, <strong>Rui Sun</strong>, Wenhao Li, Md. Atabuzzaman, Hammad Ayyubi, Haoxuan You, Alvi Md Ishmam, Kai-Wei Chang, Shih-Fu Chang, Chris Thomas<br>
			[<a href="https://journeybench.github.io/">Project Page</a>][<a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/734abb86d3caa949f44da8a093717f61-Abstract-Datasets_and_Benchmarks_Track.html">Paper</a>][<a href="https://github.com/JourneyBench/JourneyBench">Code</a>] [<a href="https://mp.weixin.qq.com/s/TmjhTEp5BkAoX18aeIms2g">量子位</a>] [<a href="https://x.com/ZCJW2021/status/1866752088077652064">Twitter</a>]<br>
			<br>
			<img src="fig/cara.jpg" alt="CARA" style="width: 550px; height: 267px;"><br>
			<b>Detecting Multimodal Situations with Insufficient Context and Abstaining from Baseless Predictions</b><br>
			ACM MM 2024<br>
			Junzhang Liu*, Zhecan Wang*, Hammad Ayyubi*, Haoxuan You, Chris Thomas, <strong>Rui Sun</strong>, Shih-Fu Chang, Kai-Wei Chang<br>
			[<a href="https://arxiv.org/abs/2405.11145">Paper</a>][<a href="https://github.com/JunzhangLiu/CARA">Code</a>]<br>
			<br>
			<img src="fig/genome.jpg" alt="GENOME" style="width: 600px; height: 300px;"><br>
			<!-- <video width="320" height="200" controls>
				<source src="https://drive.google.com/file/d/1MbTWM1QypP-aog15BeO6LxDg346r7f1I/view" type="video/mp4">
				Your browser doesn't support video label.
			</video><br> -->
			<b>GENOME: Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules</b><br>
			ICLR 2024<br>
			Zhenfang Chen*, <strong>Rui Sun*</strong>, Wenjun Liu*, Yining Hong, Chuang Gan<br>
			[<a href="https://vis-www.cs.umass.edu/genome/">Project Page</a>][<a href="https://drive.google.com/file/d/1MbTWM1QypP-aog15BeO6LxDg346r7f1I/view?usp=drive_link">Video Demo</a>][<a href="https://openreview.net/forum?id=MNShbDSxKH">Paper</a>][<a href="https://github.com/UMass-Foundation-Model/genome">Code</a>] [<a href="https://x.com/_akhaliq/status/1722472406818701724">Twitter</a>]<br>
			<!-- </div> -->
			<br>
			<!-- <div class="clearfix">
			<img src="fig/idealgpt.jpg" alt="IdealGPT" style="width: 132px; height: 66px;" class="float-left"><br> -->
			<img src="fig/idealgpt.jpg" alt="IdealGPT" style="width: 600px; height: 300px;"><br>
			<b>IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models</b><br>
			Findings of EMNLP 2023 (long)<br>
			Haoxuan You*, <strong>Rui Sun*</strong>, Zhecan Wang*, Long Chen, Gengyu Wang, Hammad A. Ayyubi, Kai-Wei Chang, Shih-Fu Chang<br>
			[<a href="https://aclanthology.org/2023.findings-emnlp.755.pdf">Paper</a>][<a href="https://github.com/Hxyou/IdealGPT">Code</a>]<br>
			<!-- </div> -->
			<br>
			<img src="fig/unifine.jpg" alt="UniFine" style="width: 387px; height: 384px;"><br>
			<b>UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding</b><br>
			Findings of ACL 2023 (long)<br>
			<strong>Rui Sun*</strong>, Zhecan Wang*, Haoxuan You*, Noel Codella, Kai-Wei Chang, Shih-Fu Chang<br>
			[<a href="https://aclanthology.org/2023.findings-acl.49/">Paper</a>][<a href="https://github.com/ThreeSR/UniFine">Code</a>]<br>
			<br>
			<img src="fig/vcg.jpg" alt="VCG" style="width: 700px; height: 300px;"><br>
			<b>Find Someone Who: Visual Commonsense Understanding in Human-Centric Grounding</b><br>
			Findings of EMNLP 2022 (long)<br>
			Haoxuan You, <strong>Rui Sun</strong>, Zhecan Wang, Kai-Wei Chang, Shih-Fu Chang<br>
			[<a href="https://aclanthology.org/2022.findings-emnlp.399/">Paper</a>][<a href="https://github.com/Hxyou/HumanCog">Code</a>]<br>
			<br><br>
			<b>Technical Reports</b><br>
			<img src="fig/second_pretrain.jpg" alt="Second_Pretrain" style="width: 650px; height: 126px;"><br>
			<b>An empirical study of QA-oriented pretraining</b><br>
			<strong>Rui Sun</strong><br>
			[<a href="https://github.com/ThreeSR/QA-Oriented-Pretraining">GitHub Repo</a>]<br>
			<img src="fig/llm_accountability.png" alt="LLM Accountability" style="width: 600px; height: 400px;"><br>
			<b>A Survey of LLM Accountability in Education, Healthcare, and Human Resources</b><br>
			Abdolrahim Arjomand, Oscar Granadino Horvath, <strong>Rui Sun†</strong>, Weihan Qu († denotes correspondence author)<br>
			[<a href="https://drive.google.com/file/d/1OFeQSeKtPY2qTvoGbnxNhq1mDIIENFmD/view?usp=sharing">Report</a>] [<a href="https://drive.google.com/file/d/16TTXbwEEyiYI785MpWzRRbG6eHIcCiDi/view?usp=sharing">Slide</a>]
		</p>
		<h3 id="exp">Experience</h3>
		<p>
			<a href="https://web.cs.ucla.edu/~kwchang/members/">UCLA NLP</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?hl=en&user=fqDBtzYAAAAJ">Prof. Kai-Wei Chang</a>, Sep 2024 -- Present<br>
			<!-- Text-to-Video Generation <br> -->
			Multimodal Language Agent <br>
			- Embodied Web Agent (Mentor: <a href="https://scholar.google.com/citations?user=PTYxORcAAAAJ">Dr. Yining Hong</a>) (In submission)<br>
			<br>
			<a href="https://mitibmwatsonailab.mit.edu/">MIT-IBM Watson AI Lab</a>, Research Assistant<br>
			Advisor & Mentor: <a href="https://scholar.google.com/citations?user=QSRdIzAAAAAJ&hl=zh-CN">Dr. Zhenfang Chen</a>, <a href="https://scholar.google.com/citations?user=PTeSCbIAAAAJ">Prof. Chuang Gan</a>, Jun 2023 -- Oct 2024<br>
			Inference Time Scaling for Language Agent with Monte Carlo Tree Search (ICLR 2025)<br>
			Neuro-Symbolic Visual Reasoning (ICLR 2024)<br>
			<br>
			<a href="https://www.ee.columbia.edu/ln/dvmm/">Digital Video and Multimedia (DVMM) Lab, Columbia University</a>, Research Assistant<br>
			Advisor & Mentor: <a href="https://scholar.google.com/citations?user=BhysChMAAAAJ&hl=en">Dr. Haoxuan You</a>, <a href="https://scholar.google.com/citations?user=uqHPnmgAAAAJ&hl=en">Dr. Zhecan Wang</a>, <a href="https://scholar.google.com/citations?user=-gtmMpIAAAAJ&hl=zh-CN">Prof. Long Chen</a>, <a href="https://scholar.google.com/citations?user=OMVTRscAAAAJ&hl=en">Prof. Shih-Fu Chang</a>, <a href="https://scholar.google.com/citations?user=fqDBtzYAAAAJ&hl=en">Prof. Kai-Wei Chang</a>, Oct 2021 -- Jun 2023, Oct 2023 -- Jun 2024<br>
			Vision and Language Understanding Evaluation (NeurIPS 2024 Datasets and Benchmarks Track)<br>
			Vision Language Models Hallucination Mitigation (ACM MM 2024)<br>
			Large Language Model Aided Visual Reasoning (Findings of EMNLP 2023)<br>
			Zero-shot Vision-Language Understanding (Findings of ACL 2023)<br>
			Human-centric Visual Commonsense Grounding Dataset (Findings of EMNLP 2022)<br>
			Task-oriented Pretraining (Second-stage Pretraining) of Vision-Language Pretrained Models (Technical Report)<br>
			<br>
			<a href="http://www1.cs.columbia.edu/nlp/index.cgi">Columbia NLP</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?user=jee2Dy0AAAAJ&hl=en">Prof. Zhou Yu</a>, Sep 2023 -- Mar 2024<br>
			<!-- Currently working on Model Editing and Fine-grained and Explainable Visual Categorization<br> -->
			Fine-grained and Explainable Visual Categorization (More details can be found in <a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3684993">this paper</a> in ACM MM)<br>
			<br>
			<a href="https://rizzolab.org/">Rehab Engineering Alliance & Center Transforming Low Vision, NYU Langone Health</a>, Research Assistant<br>
			Advisor: <a href="https://scholar.google.com/citations?user=TmPZ5uEAAAAJ&hl=en">Prof. JohnRoss Rizzo</a>, <a href="https://scholar.google.com/citations?user=jee2Dy0AAAAJ&hl=en">Prof. Zhou Yu</a>, Jun 2023 -- Aug 2023<br>
			AI for Social Good: Deploy Vision-Language & Large Language Models in wearable devices for Blindness and Low Vision People (More details can be found in <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2236097&HistoricalAwards=false">this page</a>)<br>
			<br>
			<a href="https://iip-xdu.github.io/index.html">Intelligent Information Processing (IIP) Lab, Xidian University</a>, Research Assistant<br>
			Advisor & Mentor: <a href="https://scholar.google.com/citations?user=mcIEFDMAAAAJ&hl=zh-CN">Prof. Jingwei Xin</a>, <a href="https://scholar.google.com/citations?user=SRBn7oUAAAAJ&hl">Prof. Nannan Wang</a>, Sep 2020 -- Jul 2021<br>
			Human Face Frontalization and Hallucination (More details can be found in <a href="https://ieeexplore.ieee.org/document/10115425">this paper</a> in IEEE TCSVT)<br>
		</p>
		<p>
			<img src="fig/iip_colorful.png" alt="IIP" style="width: 120px; height: 36px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/dvmm.gif" alt="DVMM" style="width: 128px; height: 36px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/nyu_langone.png" alt="NYU_Langone" style="width: 90px; height: 50px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/mit_ibm.jpg" alt="MIT_IBM" style="width: 50px; height: 50px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/columbia_nlp.jpg" alt="CU_NLP" style="width: 50px; height: 50px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/ucla_nlp.png" alt="UCLA_NLP" style="width: 50px; height: 50px;"><br>
		</p>
		<p>
			<img src="fig/xdu.jpg" alt="Xidian" style="width: 50px; height: 50px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/columbia.jpg" alt="Roar Lions!" style="width: 50px; height: 50px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="fig/ucla.png" alt="Go Bruins!" style="width: 50px; height: 50px;"><br>
		</p>
		<p>
			<img src="fig/microsoft.jpg" alt="Microsoft" style="width: 220px; height: 45px;"><br>
		</p>
		<p>
			<img src="fig/google_research.jpg" alt="Google" style="width: 220px; height: 120px;"><br>
		</p>
		<h3>Honors & Awards</h3>
		<p>
			Research fund from <a href="https://research.google/programs-and-events/research-scholar-program/recipients/?filtertab=2025">Google Research Scholar Program</a> (led by <a href="https://scholar.google.com/citations?user=16r7ZhsAAAAJ">Prof. Chris Thomas</a>), 2025 <br>
			Interdisciplinary Contest in Modeling (ICM), Meritorious Winner, 2018
		</p>
		<h3>Service</h3>
		<p>
			<!-- <b>Conference Reviewer</b><br> -->
			<b>Reviewer</b><br>
			NLP: EMNLP, EACL, NAACL, ACL Rolling Review (ARR), ACL<br>
			AI: AAAI<br>
			Multimedia: ACM MM<br>
			ML: ICLR, NeurIPS Dataset & Benchmark Track, NeurIPS Postion Paper Track<br>
			Journal: JAIR <br>
			<!-- EMNLP 2023, AAAI 2024, ACL Rolling Review (ARR), EACL 2024, NAACL 2024, ACL 2024, ACM MM 2024<br> -->
			<br>
			<b>Teaching Assistant at Columbia</b><br>
			EECS 6699 Mathematics of Deep Learning, worked with Prof. Predrag Jelenkovic, Spring 2024<br>
			COMS 4995 Deep Learning for Computer Vision, worked with Prof. Peter Belhumeur, Fall 2023<br>
			ELEN 4815 Random Signals and Noise, worked with Prof. Irving Kalet, Spring 2023<br>
			COMS 4995 Neural Networks & Deep Learning, worked with Prof. Richard Zemel, Fall 2022<br>
			COMS 4732 Computer Vision II, worked with Prof. Carl Vondrick, Spring 2022<br>
		</p>
		<h3>Misc</h3>
		<p>
			<strong>Students worked/working with me</strong> (I feel fortunate to work with these talented students)<br>
			<a href="https://maxinewu5.github.io/">Maxine Wu </a> (co-mentor with Dr. Yining Hong, UCLA Undergraduate Student)<br>
			<a href="https://alchien22.github.io/"> Alexander Chien </a> (co-mentor with Dr. Yining Hong, UCLA Undergraduate Student)<br>
		</p>
		<strong>Photos</strong><br>
		<p>
			<img src="fig/royce_hall2.jpg" alt="Fall" style="width: px; height: 227px;"> &nbsp;&nbsp;&nbsp;
			<img src="fig/irises.jpg" alt="Fall" style="width: px; height: 227px;"> &nbsp;&nbsp;&nbsp;
			<img src="fig/10km.jpg" alt="Fall" style="width: px; height: 227px;"> <br>
			Los Angeles, CA, 2024 Fall, Royce Hall <br>
			Los Angeles, CA, 2024 Fall, Getty Center <br>
			Drake Stadium, CA, 2024 Fall, My Best 10KM Record <br>
			<br>
			<a href="photos.html">Old Photos</a>
		</p>
<!-- 		<a href='https://clustrmaps.com/site/1bwuf'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=350&t=n&d=CY53cNKmeCLi9H5dBQeh2tIAkA_59a_qE7bYGFcDRrI'/></a> -->
		<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=CY53cNKmeCLi9H5dBQeh2tIAkA_59a_qE7bYGFcDRrI'></script>
	</body>
</html>
